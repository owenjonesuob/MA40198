---
title: 'MA40198: Applied Statistical Inference'
author: 'Max Anston, Owen Jones and Adam George'
header-includes:
    - \usepackage{bm}
date: "23/11/2018"
output: pdf_document
---



# Carcenogenesis Study On Rats

Load up the data and use a ggplot to see whether we expect any effect from treatment based on our data
```{r,echo}
rats <- read.table("http://people.bath.ac.uk/kai21/ASI/rats_data.txt")
library(ggplot2)
ggplot(rats, aes(group = factor(rx), fill = factor(rx), x = time)) + geom_density(alpha = 0.3)
ggplot(rats, aes(x = factor(rx), y = time, fill = factor(rx))) + geom_violin()
```
Initally it appears that treatment may be causing tumors to appear earlier in treated rats however it is hard to tell without further analysis.  

**Question 1**

The Weibull distribution is a popular distribution for looking at survival analysis, so is a natural first step for anaysing our data. First we find expressions for the Weibull Probability density and the Weibull Survival functions:
```{r}
weib_prob <- deriv(
expression(
    -log_sigma - (beta0 + beta1*treated) +
      ((1/exp(log_sigma))-1)*(log(t) - (beta0 + beta1*treated)) -
      (t/exp(beta0 + beta1*treated))^(1/exp(log_sigma))
  ),
  namevec = c("beta0", "beta1", "log_sigma"),
  function.arg = c("t", "beta0", "beta1", "log_sigma", "treated"),
  hessian = TRUE
)


weib_surv <- deriv(
  expression(
    -(t/exp(beta0 + beta1*treated))^(1/exp(log_sigma))
  ),
  namevec = c("beta0", "beta1", "log_sigma"),
  function.arg = c("t", "beta0", "beta1", "log_sigma", "treated"),
  hessian = TRUE
)
```

Then calculate the negative log likelihood and its gradient, being sure to use the survival function for censored observations.

```{r}
weib_nll <- function(theta, t, treated, status) {
  
  beta0 <- theta[1]
  beta1 <- theta[2]
  log_sigma <- theta[3]
  
  -sum(
    status * weib_prob(t, beta0, beta1, log_sigma, treated),
    (1-status) * weib_surv(t, beta0, beta1, log_sigma, treated)
  )
}
  # Gradient of negative log-likelihood
weib_nll_gr <- function(theta, t, treated, status) {

  beta0 <- theta[1]
  beta1 <- theta[2]
  log_sigma <- theta[3]

  -colSums(rbind(
    status * attr(weib_prob(t, beta0, beta1, log_sigma, treated), "gradient"),
    (1-status) * attr(weib_surv(t, beta0, beta1, log_sigma, treated), "gradient")
  ))
}
```
Now Use Optim to minimise the negative log-likelihood above
```{r}
# Initial params
theta0 <- c("beta0" = 2, "beta1" = 2, "log_sigma" = 2)


# Minimise negative log-likelihood
weib_opt <- optim(par = theta0,fn = weib_nll,gr = weib_nll_gr,t = rats$time,
            treated = rats$rx,status = rats$status,method = "BFGS",hessian = TRUE,
            control = list(trace = 1, maxit = 1000)
)
```

MLE paramater values are: Beta0 = `r (weib_opt$par[1])`, Beta1 = `r weib_opt$par[2]` and Sigma = `r exp(weib_opt$par[3])`


**Question 2**  
Use the inverse hessian calculated with optim to find the standard errors for each parametr and then use the asymptitic normal distribution to find a 95% confidence interval for $\beta_1$:

```{r}
weib_std_err <- sqrt(diag(solve(weib_opt$hessian)))
names(weib_std_err) <- c("beta0", "beta1", "log_sigma")

# 95% confidence interval for beta1
conf_beta1 <- weib_opt$par[2] + c(-1, 1)*qnorm(0.975)*weib_std_err[2]
```

Hence the 95% asymptotic confidence interval for the treatment effect $\beta_1$ is `r conf_beta1`. This interval implies that $\beta_1$ is negative. The Weibull model is influenced by $\beta_1$ in terms of $\eta_i$. If the rat has recieved treatment then $\eta_1=\beta_0+\beta_1$ if the rat is not treated then $\eta_0=\beta_0$. Therefore if we take $\beta_1$ as negative then $\eta_i$ is lower for treated rats. Then the scale parameter in the Weibull distribution is also lower as scale = exp($\eta_i$) and the exponential is an increasing function. We can work out the mean and variance of the Weibull distribution as follows:

~I THINK THIS IS WRONG, NEED TO ADD SOME MORE CONCLUSIONY STUFF HERE, QQPLOTS ETC
```{r}
weib_mean<-function(shape,scale){
  shape*gamma(1+1/shape)
}

weib_mean(1/exp(weib_opt$par[3]),exp(weib_opt$par[1]+weib_opt$par[2]))
```

**Question 3**  

We now repeat everything above but for a log-logistic distribution and its corresponding weibull function. First carry out the optimisation to find MLEs:
```{r}

# Expressions for log-logistic probability density function and log-logistic survival function
llog_prob <- deriv(
  expression(
    log((1/exp(log_sigma))/exp(beta0 + beta1*treated) * (t/exp(beta0 + beta1*treated))^((1/exp(log_sigma))-1) / (1 + (t/exp(beta0 + beta1*treated))^(1/exp(log_sigma)))^2)
  ),
  namevec = c("beta0", "beta1", "log_sigma"),
  function.arg = c("t", "beta0", "beta1", "log_sigma", "treated"),
  hessian = TRUE
)


llog_surv <- deriv(
  expression(
    -log(1 + (t/exp(beta0 + beta1*treated))^(1/exp(log_sigma)))
  ),
  namevec = c("beta0", "beta1", "log_sigma"),
  function.arg = c("t", "beta0", "beta1", "log_sigma", "treated"),
  hessian = TRUE
)



# Negative log-likelihood
llog_nll <- function(theta, t, treated, status) {

  beta0 <- theta[1]
  beta1 <- theta[2]
  log_sigma <- theta[3]

  -sum(
    status * llog_prob(t, beta0, beta1, log_sigma, treated),
    (1-status) * llog_surv(t, beta0, beta1, log_sigma, treated)
  )
}

# Gradient of negative log-likelihood
llog_nll_gr <- function(theta, t, treated, status) {

  beta0 <- theta[1]
  beta1 <- theta[2]
  log_sigma <- theta[3]

  -colSums(rbind(
    status * attr(llog_prob(t, beta0, beta1, log_sigma, treated), "gradient"),
    (1-status) * attr(llog_surv(t, beta0, beta1, log_sigma, treated), "gradient")
  ))
}




# Minimise negative log-likelihood
llog_opt <- optim(
  par = theta0,
  fn = llog_nll,
  gr = llog_nll_gr,
  t = rats$time,
  treated = rats$rx,
  status = rats$status,
  method = "BFGS",
  hessian = TRUE,
  control = list(trace = 1, maxit = 1000)
)
```

MLE paramater values are now: Beta0 = `r (llog_opt$par[1])`, Beta1 = `r llog_opt$par[2]` and Sigma = `r exp(llog_opt$par[3])` which are very similar to the values found in question 1.  

The confidence interval for $\beta_1$ is calculated as in question 2:

```{r}
# Calculate standard errors from inverse Hessian
llog_std_err <- sqrt(diag(solve(llog_opt$hessian)))

# 95% confidence interval for each parameter
# Note asymptotic distribution is normal
data.frame(par = names(llog_opt$par),
           val = llog_opt$par,
           se = llog_std_err,
           lower = llog_opt$par - qnorm(0.975)*llog_std_err,
           upper = llog_opt$par + qnorm(0.975)*llog_std_err
)
```

Hence the 95% asymptotic confidence interval for the treatment effect $\beta_1$ is `r conf_beta1`.

**Question 4**  
We now update the model to include the effect of different litters as random effects using a Weibull distribution. This accounts for any potential differences between litters, some litters may be predisposed to developing tumors earlier or later than others so adding in this random effect allows that to be accounted for. First we set up the model matricies:

```{r}
# Set up model matrices
rats$litter <- factor(rats$litter)

X <- model.matrix(~ 1 + rx, data = rats)
Z <- model.matrix(~ litter - 1, data = rats)
```

We then make a function to compute the joint logistic density of the observations and random effects:

```{r eval=FALSE}



# Expressions for model including random effects
weib_re_prob <- deriv(
  expression(
    -log_sigma - eta + (1/exp(log_sigma) - 1)*(log(t) - eta) - (t/exp(eta))^(1/exp(log_sigma))
  ),
  namevec = "eta",
  function.arg = c("t", "eta", "log_sigma"),
  hessian = TRUE
)


weib_re_surv <- deriv(
  expression(
    -(t/exp(eta))^(1/exp(log_sigma))
  ),
  namevec = "eta",
  function.arg = c("t", "eta", "log_sigma"),
  hessian = TRUE
)



# Log-likelihood function l(y) = l(y, b)
lfyb <- function(theta, y, b, X, Z) {

  beta <- theta[1:2]
  log_sigma <- theta[3]
  log_sigma_b <- theta[4]

  eta <- X%*%beta + Z%*%b

  status <- X[, 2]

  # Log conditional density of y given b
  lfy_b <- sum(
    status * weib_re_prob(y, eta, log_sigma),
    (1-status) * weib_re_surv(y, eta, log_sigma)
  )

  # Log marginal density of b
  lfb <- sum(dnorm(x = b, mean = 0, sd = exp(log_sigma_b), log = TRUE))

  # Log joint density of y and b is the sum (joint density is product - y, b are independent)
  lf <- lfy_b + lfb


  # Gradient of log-likelihood with respect to b
  # 2 terms: d(lfy_b)/db + d(lfb)/db
  g <- t(Z) %*% (status*attr(weib_re_prob(y, eta, log_sigma), "gradient") + (1-status)*attr(weib_re_surv(y, eta, log_sigma), "gradient")) - b/(exp(log_sigma_b)^2)

  # Hessian of log-likelihood wrt b i.e. dl/(db db^T)) will be diagonal since
  # taking derivative wrt bi then wrt bj where j!=i gives us 0
  # i.e. we can use "hessian" from weib_me_*()
  # So H_diag is the diagonal entries from the Hessian matrix (and all other entries are 0)
  H_diag <- t(Z) %*% (status*attr(weib_re_prob(y, eta, log_sigma), "hessian") + (1-status)*attr(weib_re_surv(y, eta, log_sigma), "hessian")) - 1/(exp(log_sigma_b)^2)

  list(lf = lf, g = g, H_diag = H_diag)

}

```

Now we compute the marginal negative log-likelihood of $\theta$ based on the laplace approximation.

```{r}
# Laplace approximation calculation of marginal negative log-likelihood of theta
lal <- function(theta, y, X, Z) {

  # Starting guess for b
  b_init <- 0.01*rnorm(ncol(Z))


  opt <- optim(
    b_init,
    fn = function(b) {
      tmp <- lfyb(theta, y, b, X, Z)
      tmp$lf
    },
    gr = function(b) {
      tmp <- lfyb(theta, y, b, X, Z)
      tmp$g
    },
    method = "BFGS",

    # Set `fnscale = -1` so that we MINIMISE the NEGATIVE log-likelihood
    control = list(fnscale = -1)
  )


  b_opt <- opt$par

  soln_opt <- lfyb(theta, y, b_opt, X, Z)

  # Hessian was diagonal, so det(-H) is product of diagonal elements
  # i.e. log(det(-H)) is sum of logs of diagonal elements
  # We use abs() since sometimes very small values have the wrong sign due to floating point errors
  log_det_H <- sum(log(abs(soln_opt$H_diag)))


  # Return approximation of log-likelihood:
  # (Use Laplace approximation to find approx marginal likelihood (integral of
  # f(y, b) over b), then take the log; we do all that in one go)
  lap <- (length(b_opt)/2)*log(2*pi) + soln_opt$lf - log_det_H/2


  # Return negative log-likelihood, and also the optimal b that gives this
  attr(lap, "b") <- b_opt
  -lap
}
```

Now use optim with the lal function to find a maximum likelihood extimate for $\theta$ with random effects.

```{r}
# Now we want to minimise negative log-likelihood (as returned by lal())
# Guess some starting values
# TODO justfy these
theta_init <- c("beta0" = 6, "beta1" = -2, "log_sigma" = -2, "log_sigma_b" = -2)


rand_opt <- optim(
  theta_init,
  fn = lal,
  y = rats$time,
  X = model.matrix(~ 1 + rx, data = rats),
  Z = model.matrix(~ litter - 1, data = rats),
  method = "BFGS",
  control = list(trace = 1, maxit = 100),
  hessian = TRUE
)


```

As before we use the hessian calculated in optim to find 95% confidence intervals for $\theta$ and specifically $\beta_1$:

```{r}
# Calculate standard errors from inverse Hessian
rand_std_err <- sqrt(diag(solve(rand_opt$hessian)))

# 95% confidence interval for each parameter
# Note asymptotic distribution is normal
data.frame(par = names(rand_opt$par),
           val = rand_opt$par,
           se = rand_std_err,
           lower = rand_opt$par - qnorm(0.975)*rand_std_err,
           upper = rand_opt$par + qnorm(0.975)*rand_std_err
)
```




